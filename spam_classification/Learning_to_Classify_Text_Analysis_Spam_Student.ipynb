{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Spam Classification\n",
    "Deciding whether an email is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#load dataset\n",
    "df=pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df=df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis='columns')\n",
    "\n",
    "#df[v1] is the class variable and df[v2] is the  email\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: removing stopwords and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "#A  stemming algorithm reduces words like fishing, fished, and fisher to the stem fish.\n",
    "#The stem need not be a word, for example  argue, argued, \n",
    "#argues, arguing, and argus could be reduced to the stem argu. \n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "#Stop words are  the most common words in a language\n",
    "#and are filtered out before processing of natural language data \n",
    "\n",
    "\n",
    "df['v2']=[re.sub('[^a-zA-Z]', ' ', sms) for sms in df['v2']]\n",
    "word_list=[sms.split() for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            updated_word=stemmer.stem(word) #stemming\n",
    "            current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri wkli comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>nd time tri contact u u pound prize claim easi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>guy bitch act like interest buy someth els nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  go jurong point crazi avail bugi n great world...\n",
       "1      ham                              ok lar joke wif u oni\n",
       "2     spam  free entri wkli comp win fa cup final tkts st ...\n",
       "3      ham                u dun say earli hor u c alreadi say\n",
       "4      ham               nah think goe usf live around though\n",
       "...    ...                                                ...\n",
       "5567  spam  nd time tri contact u u pound prize claim easi...\n",
       "5568   ham                              b go esplanad fr home\n",
       "5569   ham                                  piti mood suggest\n",
       "5570   ham  guy bitch act like interest buy someth els nex...\n",
       "5571   ham                                     rofl true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[v1] is the class variable and df[v2] is the processed email\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in training and testing\n",
    "x_train, x_test, y_train, y_test=train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: transforming email into numerical string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go jurong point crazi avail bugi n great world la e buffet cine got amor wat\n",
      "number of emails= 4457\n",
      "number of words= 5595\n",
      "  (0, 5405)\t1\n",
      "  (0, 1991)\t2\n",
      "  (0, 1140)\t1\n",
      "  (0, 3047)\t1\n",
      "  (0, 4944)\t1\n",
      "  (0, 3328)\t2\n",
      "  (0, 162)\t1\n",
      "  (0, 4483)\t1\n",
      "  (0, 1398)\t1\n",
      "  (0, 1921)\t1\n",
      "  (0, 2676)\t1\n",
      "  (0, 458)\t1\n",
      "  (0, 4620)\t1\n",
      "  (0, 1552)\t1\n",
      "  (0, 3214)\t1\n",
      "  (0, 1790)\t1\n",
      "  (0, 2541)\t1\n",
      "  (0, 4984)\t1\n",
      "  (0, 4456)\t1\n"
     ]
    }
   ],
   "source": [
    "#it counts the words\n",
    "cv=CountVectorizer()\n",
    "#it returns the number of times a word appears in the i-th email\n",
    "print(x_train[0])\n",
    "x_train_df=cv.fit_transform(x_train) #x_train_df is a matrix emails times words\n",
    "print(\"number of emails=\",x_train_df.shape[0])\n",
    "print(\"number of words=\",x_train_df.shape[1])\n",
    "x_test_df=cv.transform(x_test)\n",
    "\n",
    "\n",
    "#this is a sparse matrix (it means that only non-zeroes elements are stored)\n",
    "print(x_train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5595)\n",
      "this is the non-sparse matrix= [[0 0 0 ... 0 0 0]]\n",
      "\n",
      "wish great day moji told offer alway speechless offer easili go great length behalf stun exam next friday keep touch sorri\n",
      "\n",
      "[array(['alway', 'behalf', 'day', 'easili', 'exam', 'friday', 'go',\n",
      "       'great', 'keep', 'length', 'moji', 'next', 'offer', 'sorri',\n",
      "       'speechless', 'stun', 'told', 'touch', 'wish'], dtype='<U34')]\n",
      "\n",
      "[ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n",
      "\n",
      "[[1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "row_index=0 #select one email\n",
    "print(x_train_df[row_index,:].todense().shape)\n",
    "print(\"this is the non-sparse matrix=\",x_train_df[row_index,:].todense())\n",
    "ind=np.where(x_train_df[row_index,:].todense()[0,:]>0)[1]\n",
    "print()\n",
    "#original words in the email\n",
    "print(x_train.values[row_index])\n",
    "print()\n",
    "#decoded numerical input \n",
    "print(cv.inverse_transform(x_train_df[row_index,:].todense()))\n",
    "print()\n",
    "#index of those words in x_train_df[row_index,:].todense()\n",
    "print(ind)\n",
    "print()\n",
    "# number of times those words appears in the email\n",
    "print(x_train_df[row_index,ind].todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: training the classifier and making predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_df,y_train)\n",
    "prediction_train=clf.predict(x_train_df)\n",
    "prediction_test=clf.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: computing accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9923715503702042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_train,prediction_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about the generalisation error, that is the performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.989237668161435\n",
      "\n",
      "Confusion Matrix\n",
      "[[965   5]\n",
      " [  7 138]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_test,prediction_test)))\n",
    "print()\n",
    "\n",
    "conf_mat=confusion_matrix(y_test, prediction_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where can we find sparse matrices ?\n",
    "You can manipulate them using scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sc #this is the library\n",
    "\n",
    "#x_train_df is a scipy sparse matrix, this avoids to store the zeroes\n",
    "#to access to the non-zero element\n",
    "i=0# email index\n",
    "ind=sc.find(x_train_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_train_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [2870 3588]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set\n",
    "ind=sc.find(x_test_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_test_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider Movie Reviews Corpus, a dataset that includes  movie reviews that are categorized as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/benavoli/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "df = pd.DataFrame(columns=['v1', 'v2'])\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        df=df.append({'v1': category, 'v2': movie_reviews.words(fileid)}, ignore_index=True)\n",
    "        \n",
    "word_list=[sms for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            if word.isalpha(): #remove punctuation\n",
    "                updated_word=stemmer.stem(word) #stemming\n",
    "                current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       plot two teen coupl go church parti drink driv...\n",
       "1       happi bastard quick movi review damn bug got h...\n",
       "2       movi like make jade movi viewer thank invent t...\n",
       "3       quest camelot warner bros first featur length ...\n",
       "4       synopsi mental unstabl man undergo psychothera...\n",
       "5       capsul planet mar polic take custodi accus mur...\n",
       "6       ask eight millimet realli wholesom surveil man...\n",
       "7       exact long movi felt even nine laugh nine mont...\n",
       "8       call road trip walk wound stellan skarsg rd pl...\n",
       "9       plot young french boy see parent kill eye tim ...\n",
       "10      best rememb underst perform dr hannib lecter m...\n",
       "11      janean garofalo romant comedi good idea coupl ...\n",
       "12      high fli hong kong style filmmak made way clas...\n",
       "13      movi like mortal kombat annihil work must revi...\n",
       "14      femm la femm nikita baldwin backdraft sliver f...\n",
       "15      john carpent make b movi alway halloween escap...\n",
       "16      realli start wonder alicia silverston sure one...\n",
       "17      get mix togeth plot element various success sc...\n",
       "18      law crowd pleas romant movi state two lead mus...\n",
       "19      mighti joe young blunder near twenti minut act...\n",
       "20      spawn featur good guy bad guy lot fight bloodi...\n",
       "21      dream might keep awak night creepi imageri biz...\n",
       "22      knock exact cheap knock action movi also worst...\n",
       "23      snake eye aggrav kind movi kind show much pote...\n",
       "24      forgiv fever critic fervor crucibl infect set ...\n",
       "25      one might expect cathart view experi walk new ...\n",
       "26      america love conveni cultur invent cell phone ...\n",
       "27      reindeer game easili worst three recent film p...\n",
       "28      follow disney live action dalmatian better ent...\n",
       "29      one side doom gloom documentari possibl annihi...\n",
       "                              ...                        \n",
       "1970    synopsi phantom menac galaxi divid power group...\n",
       "1971    one point brian de palma crime epic scarfac ra...\n",
       "1972    groom verg propos marriag girlfriend certain r...\n",
       "1973    like movi albert brook realli like movi direct...\n",
       "1974    note spoiler regard film climax elect cours se...\n",
       "1975    martin scorses film use intimid reput felt obl...\n",
       "1976    robert redford river run film watch often mast...\n",
       "1977    richard gere one favorit actor howev like cour...\n",
       "1978    get jail kill star ashley judd tommi lee jone ...\n",
       "1979    let say live end airport runway larg jetlin co...\n",
       "1980    good natur pleasent easi go comedi bill murray...\n",
       "1981    man island one charact quot john donn apt pupi...\n",
       "1982    rent movi high hope movi got prais one best fi...\n",
       "1983    film divid critic consensus sharpli alan parke...\n",
       "1984    boogi night made disco respect well fashion le...\n",
       "1985    extraordinari year australian film shine scoop...\n",
       "1986    think first thing review mention wether fan x ...\n",
       "1987    tree loung director debut one favorit actor st...\n",
       "1988    wish could say someth new star war instal see ...\n",
       "1989    lisa cholodenko high art intellig quiet drama ...\n",
       "1990    relax dude ride roller coaster big lebowski fi...\n",
       "1991    box kid glove play nice nice guy never ever go...\n",
       "1992    film unexpect scari origin caught guard threw ...\n",
       "1993    plot movi take place one day rooki cop narcot ...\n",
       "1994    thriller set modern day seattl mark marki mark...\n",
       "1995    wow movi everyth movi funni dramat interest we...\n",
       "1996    richard gere command actor alway great film ev...\n",
       "1997    glori star matthew broderick denzel washington...\n",
       "1998    steven spielberg second epic film world war ii...\n",
       "1999    truman true man burbank perfect name jim carre...\n",
       "Name: v2, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same steps as in the Spam filter example, apply MultinomialNB to this example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
