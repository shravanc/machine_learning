{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./winequality.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Winedataset\n",
    "The problem of predicting quality given the wine features is a regression problem:\n",
    "* because the output (i.e., quality) is continuous \n",
    "Quality is an ordinal, that is\n",
    "\n",
    "$$\n",
    "0 \\leq 1 \\leq 2 \\leq \\dots \\leq 6\n",
    "$$\n",
    "\n",
    "a 5-quality wine is \n",
    "* better than a 4-quality wine and \n",
    "* worse than a 6-quality wine.\n",
    "\n",
    "That's the reason we formulated the problem as a regression problem, we want to predict numerical values. \n",
    "Linear regression could predict a quality that is 5.1 (even if fractional qualities are not in the dataset) and it means that wine is slightly better than a 5-quality wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now imagine we want  to consider quality a **categorical variable**, that is\n",
    "\n",
    "$$\n",
    "0,1,2,3,4,5,6\n",
    "$$\n",
    "\n",
    "are classes. This problem then becomes a classification problem\n",
    "* because the output (i.e., quality) is discrete\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Can we use MultinomialNB to predict the quality-class given the wine-characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No because wine-characteristics are continuous variables abd MultinomialNB only accepts counts.\n",
    "\n",
    "For instance, in the gender-name example, we had to use this encoding\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a &\\rightarrow & [1,0,\\dots,0]\\\\\n",
    "b &\\rightarrow & [0,1,\\dots,0]\\\\\n",
    "... & ... & ....\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so the dataset $\\{(a,1),(b,1)\\}$ is encoded into\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1,0,0,\\dots,0,1\\\\\n",
    "0,1,0,\\dots,0,1\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the last column is the class variable (gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretisation and encoding\n",
    "\n",
    "We can discretise a continuous variables by using bins (interval). Consider for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.4 7.8 7.8 ... 6.3 5.9 6. ]\n",
      "4.6 15.9\n"
     ]
    }
   ],
   "source": [
    "fixed_ac = dataset['fixed acidity'].values\n",
    "print(fixed_ac)\n",
    "print(np.min(fixed_ac),np.max(fixed_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the interval $[4.6, 15.9]$ in five equally spaced intervals, e.g.,\n",
    "\n",
    "$$\n",
    "[ 4.6 ,  6.86], ~~[6.86,  9.12], ~~[9.12, 11.38], ~~[11.38,13.64], ~~[13.64, 15.9 ]\n",
    "$$\n",
    "\n",
    "and we can consider these five intervals as five classes.\n",
    "This is what we do for instance when we plot histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([274., 913., 298., 102.,  12.]),\n",
       " array([ 4.6 ,  6.86,  9.12, 11.38, 13.64, 15.9 ]),\n",
       " <a list of 5 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Biining a continuous variable\n",
    "plt.hist(fixed_ac,bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a faster way to that using a function called `KBinsDiscretizer` from `sklearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.4]\n",
      " [7.8]\n",
      " [7.8]\n",
      " ...\n",
      " [6.3]\n",
      " [5.9]\n",
      " [6. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fixed_ac.reshape(-1,1)\n",
    "print(X)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='uniform')\n",
    "est.fit(X)\n",
    "X_d = est.transform(X)\n",
    "X_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99],\n",
       "       [7.99],\n",
       "       [7.99],\n",
       "       ...,\n",
       "       [5.73],\n",
       "       [5.73],\n",
       "       [5.73]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.inverse_transform(X_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.73]]\n",
      "[[7.99]]\n",
      "[[10.25]]\n",
      "[[12.51]]\n",
      "[[14.77]]\n"
     ]
    }
   ],
   "source": [
    "print(est.inverse_transform(np.array([[0]])))\n",
    "print(est.inverse_transform(np.array([[1]])))\n",
    "print(est.inverse_transform(np.array([[2]])))\n",
    "print(est.inverse_transform(np.array([[3]])))\n",
    "print(est.inverse_transform(np.array([[4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to apply MultinomialNB we need\n",
    "1. to discretize the variables in 5 categories (0,1,2,3,4)\n",
    "2. to encode the 5 categories\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &\\rightarrow & [1,0,0,0,0]\\\\\n",
    "1 &\\rightarrow & [0,1,0,0,0]\\\\\n",
    "2 &\\rightarrow & [0,0,1,0,0]\\\\\n",
    "3 &\\rightarrow & [0,0,0,1,0]\\\\\n",
    "4 &\\rightarrow & [0,0,0,0,1]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can do it in one-shot by using `KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')`\n",
    "In this way, the inputs are already in the right format for MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1599x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fixed_ac.reshape(-1,1)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')\n",
    "est.fit(X)\n",
    "X_d = est.transform(X)\n",
    "X_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`Xd_train` is saved as a sparse-matrix so we do not need to save all the zeros and in this way we save memory\n",
    "space. If we want to see it, we can transform it back to a normal (dense) matrix with the following instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can transforma sparse matrix into a normal matrix as:\n",
    "X_d.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step1: we divide the dataset in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(959, 11)\n",
      "(640, 11)\n"
     ]
    }
   ],
   "source": [
    "#we split the dataset in training and testing\n",
    "X=dataset.iloc[:,0:-1].values\n",
    "y=dataset.iloc[:,-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step2: we discretize the inputs in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<959x55 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10549 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')\n",
    "est.fit(X_train)\n",
    "Xd_train = est.transform(X_train)\n",
    "Xd_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step3: we discretize the inputs in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_test  = est.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we  **don't do**\n",
    "\n",
    "`est = KBinsDiscretizer(n_bins=10,encode='onehot',strategy='quantile')\n",
    "est.fit(X_test)\n",
    "Xd_test = est.transform(X_test)\n",
    "Xd_train`\n",
    "\n",
    "A discretisation algorithm is a ML algorithm and, therefore, we do `fit` only on the training dataset and we only transform the test dataset. Otherwise, we could risk overfitting.\n",
    "\n",
    "This is a **general approach**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General ML Recipe algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() # \n",
    "\n",
    "#training\n",
    "clf.fit(Xd_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(Xd_train)\n",
    "y_test_pred  = clf.predict(Xd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.5901981230448383\n",
      "accuracy test set  0.5640625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(\"accuracy training set \", accuracy_score(y_train,y_train_pred))\n",
    "print(\"accuracy test set \",accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix test set \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   1,   0,   0,   0],\n",
       "       [  0,   2,  11,  10,   0,   0],\n",
       "       [  0,   1, 190,  78,   7,   2],\n",
       "       [  0,   1,  88, 126,  32,   1],\n",
       "       [  0,   0,   5,  32,  43,   1],\n",
       "       [  0,   0,   0,   4,   4,   0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion Matrix test set \")\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question \n",
    "How do we select the number of bins in `KBinsDiscretizer(n_bins=7,encode='onehot',strategy='quantile')`?\n",
    "\n",
    "Is there an optimal way (the discretisation that gives us the highest accuracy on the test set)?\n",
    "\n",
    "To answer this question we can use 10-folds cross-validation. This will help us\n",
    "to avoid overfitting. We select the value n_bins that provides the highest accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "CV.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"CV.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop computes the 10-fold cross-validation accuracy for a fixed value of `n_bins`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy= 0.5660062893081761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "n_bins = 4\n",
    "k_fold = KFold(n_splits=10,  shuffle=True, random_state=42)\n",
    "Accuracy=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy=\",np.mean(np.array(Accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Implement a for-loop that tries all the values of `n_bins` in the  `range(2,20)`  and\n",
    "selects the value `n_bins` corresponding to the highest  best **Average 10-fold cross-validation accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression classifier\n",
    "\n",
    "Can we use a classifier than doesn't need discretisation?\n",
    "Equivalently, can we use directly continuous inputs without discretisation?\n",
    "\n",
    "\n",
    "Logistic Regression classifier is the simplest neural network. It is a classifier that directly accepts\n",
    "continuous inputs. We will go into details during the lecture on Monday; we will use it now as a blackbox model,\n",
    "that is without going into details of the algorithm.\n",
    "\n",
    "Note that, although is called **Logistic Regression** it is actually  a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shravan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#we split the dataset in training and testing\n",
    "X=dataset.iloc[:,0:-1].values\n",
    "y=dataset.iloc[:,-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "#Logistic\n",
    "clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                   #has more than two categories\n",
    "clf.fit(X_train,y_train)\n",
    "y_train_pred2= clf.predict(X_train)\n",
    "y_test_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.5922836287799792\n",
      "accuracy test set  0.5546875\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy training set \", accuracy_score(y_train_pred2,y_train))\n",
    "print(\"accuracy test set \",accuracy_score(y_test_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving the convergence problem by normalsing the inputs\n",
    "When you run LogisticRegression you have this warning\n",
    "\n",
    "`...python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
    "  \"of iterations.\", ConvergenceWarning)\n",
    "`\n",
    "\n",
    "This is a numerical problem, the optimiser lbfgs, that numerically maximises the likelihood to find the MLE, is not converging (more details on Monday). \n",
    "\n",
    "This is a common problem in ML. A way to help lbfgs is by scaling the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "scalerX.fit(X_train)\n",
    "X_tr_scaled = scalerX.transform(X_train)\n",
    "X_te_scaled = scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard scaler normalize the data, that is for each column in X_train does the following\n",
    "\n",
    "`\n",
    "(X_train[:,i]-np.mean(X_train[:,i]))/np.std(X_train[:,i])\n",
    "`\n",
    "\n",
    "where `mean` computes the mean and `std` the standard deviation of the column.\n",
    "\n",
    "Example: assume the data is $[10,50,35,20]$, then \n",
    "\n",
    "$$\n",
    "mean=\\frac{10+50+35+20}{4}=28.75\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "std=\\sqrt{\\frac{(10-28.75)^2+(50-28.75)^2+(35-28.75)^2+(20-28.75)^2}{4}}=1.5155\n",
    "$$\n",
    "\n",
    "and so the transformed data is\n",
    "\n",
    "$$\n",
    "\\frac{[10,50,35,20]-28.75}{15.155}=[-1.23717915,  1.40213637,  0.41239305, -0.57735027]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQOUlEQVR4nO3dcayddX3H8fdnhYGZbsC4Y13b7BLXzaCZxdxVDPuDwZwFjMVlEkimzJHUJZBgYuaK/qEmI8Fsyma2sVRh1I2JjUpoBKcVSYjJAC9YK6UyOy2hTaVXEYSQsbV898d9qsdy23PuPffe0/vr+5WcnOf5/X7Peb6X3Hz68Du/57mpKiRJbfmFURcgSZp/hrskNchwl6QGGe6S1CDDXZIadNKoCwA488wza3x8fNRlSNKS8vDDD/+wqsZm6jsuwn18fJzJyclRlyFJS0qSJ47W57SMJDXIcJekBhnuktSgvuGe5NQkDyX5VpKdST7Std+W5PtJtnevNV17knwiye4kO5K8YaF/CEnSzxvkC9UXgQur6vkkJwNfT/Klru8vq+pzR4y/GFjdvd4I3Ny9S5IWSd8r95r2fLd7cvc61tPG1gOf7o57ADgtyfLhS5UkDWqgOfcky5JsBw4A26rqwa7rhm7q5aYkp3RtK4Anew7f27Ud+ZkbkkwmmZyamhriR5AkHWmgcK+qQ1W1BlgJrE3yOuB64DXA7wFnAH81mxNX1aaqmqiqibGxGdfgS5LmaFarZarqGeA+YF1V7e+mXl4E/gVY2w3bB6zqOWxl1yZJWiR9v1BNMgb8X1U9k+QVwJuBjyZZXlX7kwS4DHi0O2QrcG2SO5j+IvXZqtq/QPWfkMY33j2yc++58dKRnVvS4AZZLbMc2JxkGdNX+luq6otJvtYFf4DtwF904+8BLgF2Ay8A757/siVJx9I33KtqB3DuDO0XHmV8AdcMX5okaa68Q1WSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQX3DPcmpSR5K8q0kO5N8pGs/O8mDSXYn+WySX+zaT+n2d3f94wv7I0iSjjTIlfuLwIVV9XpgDbAuyXnAR4Gbquq3gB8DV3fjrwZ+3LXf1I2TJC2ivuFe057vdk/uXgVcCHyua98MXNZtr+/26fovSpJ5q1iS1NdAc+5JliXZDhwAtgH/DTxTVQe7IXuBFd32CuBJgK7/WeBXZ/jMDUkmk0xOTU0N91NIkn7OQOFeVYeqag2wElgLvGbYE1fVpqqaqKqJsbGxYT9OktRjVqtlquoZ4D7gTcBpSU7qulYC+7rtfcAqgK7/V4AfzUu1kqSBDLJaZizJad32K4A3A7uYDvk/6YZdBdzVbW/t9un6v1ZVNZ9FS5KO7aT+Q1gObE6yjOl/DLZU1ReTPAbckeSvgW8Ct3TjbwH+Nclu4GngigWoW5J0DH3Dvap2AOfO0P49puffj2z/H+Ad81KdJGlOvENVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUF9wz3JqiT3JXksyc4k13XtH06yL8n27nVJzzHXJ9md5PEkb1nIH0CS9HInDTDmIPC+qnokyauAh5Ns6/puqqq/7R2c5BzgCuC1wG8AX03y21V1aD4LlyQdXd8r96raX1WPdNvPAbuAFcc4ZD1wR1W9WFXfB3YDa+ejWEnSYGY1555kHDgXeLBrujbJjiS3Jjm9a1sBPNlz2F5m+McgyYYkk0kmp6amZl24JOnoBg73JK8EPg+8t6p+AtwMvBpYA+wHPjabE1fVpqqaqKqJsbGx2RwqSepjoHBPcjLTwX57VX0BoKqeqqpDVfUS8El+NvWyD1jVc/jKrk2StEgGWS0T4BZgV1V9vKd9ec+wtwOPdttbgSuSnJLkbGA18ND8lSxJ6meQ1TLnA+8Evp1ke9f2AeDKJGuAAvYA7wGoqp1JtgCPMb3S5hpXykjS4uob7lX1dSAzdN1zjGNuAG4Yoi5J0hC8Q1WSGmS4S1KDDHdJatAgX6hKPzW+8e6RnHfPjZeO5LzSUuWVuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWob7gnWZXkviSPJdmZ5Lqu/Ywk25J8t3s/vWtPkk8k2Z1kR5I3LPQPIUn6eYNcuR8E3ldV5wDnAdckOQfYCNxbVauBe7t9gIuB1d1rA3DzvFctSTqmvuFeVfur6pFu+zlgF7ACWA9s7oZtBi7rttcDn65pDwCnJVk+75VLko5qVnPuScaBc4EHgbOqan/X9QPgrG57BfBkz2F7u7YjP2tDkskkk1NTU7MsW5J0LAOHe5JXAp8H3ltVP+ntq6oCajYnrqpNVTVRVRNjY2OzOVSS1MdA4Z7kZKaD/faq+kLX/NTh6Zbu/UDXvg9Y1XP4yq5NkrRIBlktE+AWYFdVfbynaytwVbd9FXBXT/u7ulUz5wHP9kzfSJIWwUkDjDkfeCfw7STbu7YPADcCW5JcDTwBXN713QNcAuwGXgDePa8VS5L66hvuVfV1IEfpvmiG8QVcM2RdkqQheIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoP6hnuSW5McSPJoT9uHk+xLsr17XdLTd32S3UkeT/KWhSpcknR0g1y53wasm6H9pqpa073uAUhyDnAF8NrumH9Ksmy+ipUkDaZvuFfV/cDTA37eeuCOqnqxqr4P7AbWDlGfJGkOhplzvzbJjm7a5vSubQXwZM+YvV3byyTZkGQyyeTU1NQQZUiSjjTXcL8ZeDWwBtgPfGy2H1BVm6pqoqomxsbG5liGJGkmcwr3qnqqqg5V1UvAJ/nZ1Ms+YFXP0JVdmyRpEc0p3JMs79l9O3B4Jc1W4IokpyQ5G1gNPDRciZKk2Tqp34AknwEuAM5Mshf4EHBBkjVAAXuA9wBU1c4kW4DHgIPANVV1aGFKlyQdTd9wr6orZ2i+5RjjbwBuGKYoSdJwvENVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3q++AwHd34xrtHXYIkzcgrd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGuQ6dy0Jo7ynYM+Nl47s3NJc9b1yT3JrkgNJHu1pOyPJtiTf7d5P79qT5BNJdifZkeQNC1m8JGlmg0zL3AasO6JtI3BvVa0G7u32AS4GVnevDcDN81OmJGk2+oZ7Vd0PPH1E83pgc7e9Gbisp/3TNe0B4LQky+erWEnSYOb6hepZVbW/2/4BcFa3vQJ4smfc3q7tZZJsSDKZZHJqamqOZUiSZjL0apmqKqDmcNymqpqoqomxsbFhy5Ak9ZhruD91eLqlez/Qte8DVvWMW9m1SZIW0VzDfStwVbd9FXBXT/u7ulUz5wHP9kzfSJIWSd917kk+A1wAnJlkL/Ah4EZgS5KrgSeAy7vh9wCXALuBF4B3L0DNkqQ++oZ7VV15lK6LZhhbwDXDFiVJGo6PH5CkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBfZ/nLp3oxjfePZLz7rnx0pGcV23wyl2SGmS4S1KDDHdJapDhLkkNMtwlqUFDrZZJsgd4DjgEHKyqiSRnAJ8FxoE9wOVV9ePhypQkzcZ8XLn/QVWtqaqJbn8jcG9VrQbu7fYlSYtoIaZl1gObu+3NwGULcA5J0jEMG+4FfCXJw0k2dG1nVdX+bvsHwFkzHZhkQ5LJJJNTU1NDliFJ6jXsHaq/X1X7kvwasC3Jd3o7q6qS1EwHVtUmYBPAxMTEjGMkSXMz1JV7Ve3r3g8AdwJrgaeSLAfo3g8MW6QkaXbmHO5JfinJqw5vA38EPApsBa7qhl0F3DVskZKk2RlmWuYs4M4khz/n36vqP5J8A9iS5GrgCeDy4cuUJM3GnMO9qr4HvH6G9h8BFw1TlCSfRqnheIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGvbZMiM3qrXAknQ888pdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KAlfxOTpPk1yhsD/UMh88crd0lqkOEuSQ0y3CWpQYa7JDVowb5QTbIO+HtgGfCpqrpxoc4lScNo8UvkBblyT7IM+EfgYuAc4Mok5yzEuSRJL7dQV+5rgd1V9T2AJHcA64HHFuh8khrg32eYPwsV7iuAJ3v29wJv7B2QZAOwodt9PsnjC1QLwJnADxfw8xeCNS+epVi3NS+OBa85Hx3q8N88WsfIbmKqqk3ApsU4V5LJqppYjHPNF2tePEuxbmteHEux5sMWarXMPmBVz/7Krk2StAgWKty/AaxOcnaSXwSuALYu0LkkSUdYkGmZqjqY5Frgy0wvhby1qnYuxLkGtCjTP/PMmhfPUqzbmhfHUqwZgFTVqGuQJM0z71CVpAYZ7pLUoBMm3JP8TZLvJNmR5M4kp426pn6SvCPJziQvJTmul2MlWZfk8SS7k2wcdT2DSHJrkgNJHh11LYNIsirJfUke634vrht1TYNIcmqSh5J8q6v7I6OuaRBJliX5ZpIvjrqWuThhwh3YBryuqn4X+C/g+hHXM4hHgT8G7h91IceyhB83cRuwbtRFzMJB4H1VdQ5wHnDNEvnv/CJwYVW9HlgDrEty3ohrGsR1wK5RFzFXJ0y4V9VXqupgt/sA02vvj2tVtauqFvLO3fny08dNVNX/AocfN3Fcq6r7gadHXcegqmp/VT3SbT/HdPCsGG1V/dW057vdk7vXcb2SI8lK4FLgU6OuZa5OmHA/wp8DXxp1EQ2Z6XETx33oLGVJxoFzgQdHW8lguimO7cABYFtVHe91/x3wfuClURcyV039DdUkXwV+fYauD1bVXd2YDzL9v7e3L2ZtRzNIzVKvJK8EPg+8t6p+Mup6BlFVh4A13XdddyZ5XVUdl991JHkrcKCqHk5ywajrmaumwr2q/vBY/Un+DHgrcFEdJwv8+9W8RPi4iUWS5GSmg/32qvrCqOuZrap6Jsl9TH/XcVyGO3A+8LYklwCnAr+c5N+q6k9HXNesnDDTMt0fD3k/8LaqemHU9TTGx00sgiQBbgF2VdXHR13PoJKMHV6dluQVwJuB74y2qqOrquuramVVjTP9u/y1pRbscAKFO/APwKuAbUm2J/nnURfUT5K3J9kLvAm4O8mXR13TTLovqg8/bmIXsGXEj5sYSJLPAP8J/E6SvUmuHnVNfZwPvBO4sPsd3t5dXR7vlgP3JdnB9IXAtqpakssLlxIfPyBJDTqRrtwl6YRhuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG/T96o1kAYvJbxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col=0\n",
    "#before\n",
    "plt.hist(X_train[:,col]);\n",
    "plt.figure()\n",
    "#after\n",
    "plt.hist(X_tr_scaled[:,col]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.6308654848800834\n",
      "accuracy test set  0.56875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                   #has more than two categories\n",
    "clf.fit(X_tr_scaled,y_train)\n",
    "y_train_pred2= clf.predict(X_tr_scaled)\n",
    "y_test_pred2 = clf.predict(X_te_scaled)\n",
    "print(\"accuracy training set \", accuracy_score(y_train_pred2,y_train))\n",
    "print(\"accuracy test set \",accuracy_score(y_test_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  the classifier has  a better perfomance, because lbfgs computed a better solution (We will go into details on Monday).\n",
    "\n",
    "Is scaling+LogisticRegression better than discretisation + MultinomialNB?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "Use 10-fold cross validation to compare the performance of the two classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Business Problem: Banks receives so many applications for credit card (CC) request. Going through each request manually can be very time consuming, also prone to human errors. However, if we can use the historical data to build a model which can shortlist the candidates for approval that save time.\n",
    "\n",
    "By using the previous two classifiers, you must build a predictor model that predicts if a CC should be approved\n",
    "for a person that has certain characteristics (sex,age,Debt, Married etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Married</th>\n",
       "      <th>BankCustomer</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>YearsEmployed</th>\n",
       "      <th>PriorDefault</th>\n",
       "      <th>Employed</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Citizen</th>\n",
       "      <th>Income</th>\n",
       "      <th>Approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>32.08</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>33.17</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>22.92</td>\n",
       "      <td>11.585</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>54.42</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3.960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>42.50</td>\n",
       "      <td>4.915</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3.165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>29.92</td>\n",
       "      <td>1.835</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.335</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>38.25</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>48.08</td>\n",
       "      <td>6.040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>45.83</td>\n",
       "      <td>10.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>36.67</td>\n",
       "      <td>4.415</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>28.25</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>23.25</td>\n",
       "      <td>5.875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3.170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>21.83</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>19.17</td>\n",
       "      <td>8.585</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>25.00</td>\n",
       "      <td>11.250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>47.75</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>27.42</td>\n",
       "      <td>14.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3.085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>41.17</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>15.83</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>13.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>56.58</td>\n",
       "      <td>18.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>57.42</td>\n",
       "      <td>8.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>42.08</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1</td>\n",
       "      <td>22.25</td>\n",
       "      <td>9.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>1</td>\n",
       "      <td>29.83</td>\n",
       "      <td>3.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0</td>\n",
       "      <td>23.50</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>1</td>\n",
       "      <td>32.08</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>1</td>\n",
       "      <td>31.08</td>\n",
       "      <td>1.500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1</td>\n",
       "      <td>31.83</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>11.750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>1</td>\n",
       "      <td>30.33</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>1</td>\n",
       "      <td>51.83</td>\n",
       "      <td>2.040</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>1</td>\n",
       "      <td>47.17</td>\n",
       "      <td>5.835</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1</td>\n",
       "      <td>25.83</td>\n",
       "      <td>12.835</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0</td>\n",
       "      <td>50.25</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1</td>\n",
       "      <td>29.50</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0</td>\n",
       "      <td>37.33</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0</td>\n",
       "      <td>41.58</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0</td>\n",
       "      <td>30.58</td>\n",
       "      <td>10.665</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>1</td>\n",
       "      <td>19.42</td>\n",
       "      <td>7.250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0</td>\n",
       "      <td>17.92</td>\n",
       "      <td>10.210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0</td>\n",
       "      <td>20.08</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>1</td>\n",
       "      <td>19.50</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1</td>\n",
       "      <td>17.08</td>\n",
       "      <td>3.290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>1</td>\n",
       "      <td>36.42</td>\n",
       "      <td>0.750</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>1</td>\n",
       "      <td>40.58</td>\n",
       "      <td>3.290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>1</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Male    Age    Debt  Married  BankCustomer  EducationLevel  Ethnicity  \\\n",
       "0       1  30.83   0.000        1             0              12          7   \n",
       "1       0  58.67   4.460        1             0              10          3   \n",
       "2       0  24.50   0.500        1             0              10          3   \n",
       "3       1  27.83   1.540        1             0              12          7   \n",
       "4       1  20.17   5.625        1             0              12          7   \n",
       "5       1  32.08   4.000        1             0               9          7   \n",
       "6       1  33.17   1.040        1             0              11          3   \n",
       "7       0  22.92  11.585        1             0               2          7   \n",
       "8       1  54.42   0.500        2             2               8          3   \n",
       "9       1  42.50   4.915        2             2              12          7   \n",
       "10      1  22.08   0.830        1             0               1          3   \n",
       "11      1  29.92   1.835        1             0               1          3   \n",
       "12      0  38.25   6.000        1             0               8          7   \n",
       "13      1  48.08   6.040        1             0               8          7   \n",
       "14      0  45.83  10.500        1             0              10          7   \n",
       "15      1  36.67   4.415        2             2               8          7   \n",
       "16      1  28.25   0.875        1             0               9          7   \n",
       "17      0  23.25   5.875        1             0              10          7   \n",
       "18      1  21.83   0.250        1             0               3          3   \n",
       "19      0  19.17   8.585        1             0               2          3   \n",
       "20      1  25.00  11.250        1             0               1          7   \n",
       "21      1  23.25   1.000        1             0               1          7   \n",
       "22      0  47.75   8.000        1             0               1          7   \n",
       "23      0  27.42  14.500        1             0              13          3   \n",
       "24      0  41.17   6.500        1             0              10          7   \n",
       "25      0  15.83   0.585        1             0               1          3   \n",
       "26      0  47.00  13.000        1             0               6          0   \n",
       "27      1  56.58  18.500        1             0               3          0   \n",
       "28      1  57.42   8.500        1             0               4          3   \n",
       "29      1  42.08   1.040        1             0              12          7   \n",
       "..    ...    ...     ...      ...           ...             ...        ...   \n",
       "660     1  22.25   9.000        1             0               0          7   \n",
       "661     1  29.83   3.500        1             0               1          7   \n",
       "662     0  23.50   1.500        1             0              12          7   \n",
       "663     1  32.08   4.000        2             2               2          7   \n",
       "664     1  31.08   1.500        2             2              12          7   \n",
       "665     1  31.83   0.040        2             2               9          7   \n",
       "666     0  21.75  11.750        1             0               1          7   \n",
       "667     0  17.92   0.540        1             0               1          7   \n",
       "668     1  30.33   0.500        1             0               3          3   \n",
       "669     1  51.83   2.040        2             2               5          2   \n",
       "670     1  47.17   5.835        1             0              12          7   \n",
       "671     1  25.83  12.835        1             0               2          7   \n",
       "672     0  50.25   0.835        1             0               0          7   \n",
       "673     1  29.50   2.000        2             2               4          3   \n",
       "674     0  37.33   2.500        1             0               6          3   \n",
       "675     0  41.58   1.040        1             0               0          7   \n",
       "676     0  30.58  10.665        1             0              10          3   \n",
       "677     1  19.42   7.250        1             0               9          7   \n",
       "678     0  17.92  10.210        1             0               5          2   \n",
       "679     0  20.08   1.250        1             0               1          7   \n",
       "680     1  19.50   0.290        1             0               8          7   \n",
       "681     1  27.83   1.000        2             2               3          3   \n",
       "682     1  17.08   3.290        1             0               6          7   \n",
       "683     1  36.42   0.750        2             2               3          7   \n",
       "684     1  40.58   3.290        1             0               9          7   \n",
       "685     1  21.08  10.085        2             2               4          3   \n",
       "686     0  22.67   0.750        1             0               1          7   \n",
       "687     0  25.25  13.500        2             2               5          2   \n",
       "688     1  17.92   0.205        1             0               0          7   \n",
       "689     1  35.00   3.375        1             0               1          3   \n",
       "\n",
       "     YearsEmployed  PriorDefault  Employed  CreditScore  Citizen  Income  \\\n",
       "0            1.250             1         1            1        0       0   \n",
       "1            3.040             1         1            6        0     560   \n",
       "2            1.500             1         0            0        0     824   \n",
       "3            3.750             1         1            5        0       3   \n",
       "4            1.710             1         0            0        2       0   \n",
       "5            2.500             1         0            0        0       0   \n",
       "6            6.500             1         0            0        0   31285   \n",
       "7            0.040             1         0            0        0    1349   \n",
       "8            3.960             1         0            0        0     314   \n",
       "9            3.165             1         0            0        0    1442   \n",
       "10           2.165             0         0            0        0       0   \n",
       "11           4.335             1         0            0        0     200   \n",
       "12           1.000             1         0            0        0       0   \n",
       "13           0.040             0         0            0        0    2690   \n",
       "14           5.000             1         1            7        0       0   \n",
       "15           0.250             1         1           10        0       0   \n",
       "16           0.960             1         1            3        0       0   \n",
       "17           3.170             1         1           10        0     245   \n",
       "18           0.665             1         0            0        0       0   \n",
       "19           0.750             1         1            7        0       0   \n",
       "20           2.500             1         1           17        0    1208   \n",
       "21           0.835             1         0            0        2       0   \n",
       "22           7.875             1         1            6        0    1260   \n",
       "23           3.085             1         1            1        0      11   \n",
       "24           0.500             1         1            3        0       0   \n",
       "25           1.500             1         1            2        0       0   \n",
       "26           5.165             1         1            9        0       0   \n",
       "27          15.000             1         1           17        0       0   \n",
       "28           7.000             1         1            3        0       0   \n",
       "29           5.000             1         1            6        0   10000   \n",
       "..             ...           ...       ...          ...      ...     ...   \n",
       "660          0.085             0         0            0        0       0   \n",
       "661          0.165             0         0            0        0       0   \n",
       "662          0.875             0         0            0        0       0   \n",
       "663          1.500             0         0            0        0       0   \n",
       "664          0.040             0         0            0        2       0   \n",
       "665          0.040             0         0            0        0       0   \n",
       "666          0.250             0         0            0        0       0   \n",
       "667          1.750             0         1            1        0       5   \n",
       "668          0.085             0         0            0        2       0   \n",
       "669          1.500             0         0            0        0       1   \n",
       "670          5.500             0         0            0        0     150   \n",
       "671          0.500             0         0            0        0       2   \n",
       "672          0.500             0         0            0        0     117   \n",
       "673          2.000             0         0            0        0      17   \n",
       "674          0.210             0         0            0        0     246   \n",
       "675          0.665             0         0            0        0     237   \n",
       "676          0.085             0         1           12        0       3   \n",
       "677          0.040             0         1            1        0       1   \n",
       "678          0.000             0         0            0        0      50   \n",
       "679          0.000             0         0            0        0       0   \n",
       "680          0.290             0         0            0        0     364   \n",
       "681          3.000             0         0            0        0     537   \n",
       "682          0.335             0         0            0        0       2   \n",
       "683          0.585             0         0            0        0       3   \n",
       "684          3.500             0         0            0        2       0   \n",
       "685          1.250             0         0            0        0       0   \n",
       "686          2.000             0         1            2        0     394   \n",
       "687          2.000             0         1            1        0       1   \n",
       "688          0.040             0         0            0        0     750   \n",
       "689          8.290             0         0            0        0       0   \n",
       "\n",
       "     Approved  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  \n",
       "10          0  \n",
       "11          0  \n",
       "12          0  \n",
       "13          0  \n",
       "14          0  \n",
       "15          0  \n",
       "16          0  \n",
       "17          0  \n",
       "18          0  \n",
       "19          0  \n",
       "20          0  \n",
       "21          0  \n",
       "22          0  \n",
       "23          0  \n",
       "24          0  \n",
       "25          0  \n",
       "26          0  \n",
       "27          0  \n",
       "28          0  \n",
       "29          0  \n",
       "..        ...  \n",
       "660         1  \n",
       "661         1  \n",
       "662         1  \n",
       "663         1  \n",
       "664         1  \n",
       "665         1  \n",
       "666         1  \n",
       "667         1  \n",
       "668         1  \n",
       "669         1  \n",
       "670         1  \n",
       "671         1  \n",
       "672         1  \n",
       "673         1  \n",
       "674         1  \n",
       "675         1  \n",
       "676         1  \n",
       "677         1  \n",
       "678         1  \n",
       "679         1  \n",
       "680         1  \n",
       "681         1  \n",
       "682         1  \n",
       "683         1  \n",
       "684         1  \n",
       "685         1  \n",
       "686         1  \n",
       "687         1  \n",
       "688         1  \n",
       "689         1  \n",
       "\n",
       "[690 rows x 14 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/CreditCard_cleanedData.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new variable to input features and labels\n",
    "X,y = df.iloc[:,0:13].values , df.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Exercise 1:\n",
    "the best model is n_bins = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.          0.51090016]\n",
      " [ 3.          0.53285377]\n",
      " [ 4.          0.55407233]\n",
      " [ 5.          0.55155267]\n",
      " [ 6.          0.54847091]\n",
      " [ 7.          0.56910377]\n",
      " [ 8.          0.55595519]\n",
      " [ 9.          0.56159984]\n",
      " [10.          0.55720519]\n",
      " [11.          0.55660377]\n",
      " [12.          0.55720519]\n",
      " [13.          0.56722484]\n",
      " [14.          0.56784591]\n",
      " [15.          0.55156447]\n",
      " [16.          0.56533805]\n",
      " [17.          0.54782626]\n",
      " [18.          0.55032626]\n",
      " [19.          0.5559316 ]]\n",
      "Best n_bins= [7.         0.56910377]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "Accuracy_Fold=[]\n",
    "for n_bins in range(2,20):\n",
    "    Accuracy=[]\n",
    "    for train_indices, test_indices in k_fold.split(X):\n",
    "        #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "        X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "        #discretise\n",
    "        est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "        est.fit(X_train)\n",
    "        Xd_train = est.transform(X_train)\n",
    "        Xd_test  = est.transform(X_test)\n",
    "        clf = MultinomialNB() \n",
    "        #training\n",
    "        clf.fit(Xd_train, y_train)\n",
    "\n",
    "        #prediction\n",
    "        y_test_pred  = clf.predict(Xd_test)\n",
    "        #accuracy\n",
    "        Accuracy.append(accuracy_score(y_test,y_test_pred))\n",
    "    #print(\"Average 10-fold cross-validation accuracy=\",np.mean(np.array(Accuracy)))\n",
    "    Accuracy_Fold.append([n_bins,np.mean(np.array(Accuracy))])\n",
    "Accuracy_Fold=np.array(Accuracy_Fold)\n",
    "print(Accuracy_Fold)\n",
    "ind=np.argmax(Accuracy_Fold[:,1])\n",
    "print(\"Best n_bins=\",Accuracy_Fold[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "n_bins = 7\n",
    "\n",
    "Accuracy_MNB=[]\n",
    "Accuracy_Logistic=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy_MNB.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_train)\n",
    "    X_tr_scaled = scalerX.transform(X_train)\n",
    "    X_te_scaled = scalerX.transform(X_test)\n",
    "    \n",
    "    clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                    #has more than two categories\n",
    "    clf.fit(X_tr_scaled,y_train)\n",
    "\n",
    "    y_test_pred_logistic = clf.predict(X_te_scaled)\n",
    "    Accuracy_Logistic.append(accuracy_score(y_test,y_test_pred_logistic))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy MultinomialNB=\",np.mean(np.array(Accuracy_MNB)))\n",
    "print(\"Average 10-fold cross-validation accuracy Logistic=\",np.mean(np.array(Accuracy_Logistic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 (same solution has before, but one should first select the optimal number of bins\n",
    "as in Exercise 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy MultinomialNB= 0.8391304347826087\n",
      "Average 10-fold cross-validation accuracy Logistic= 0.8405797101449277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "n_bins = 7\n",
    "\n",
    "Accuracy_MNB=[]\n",
    "Accuracy_Logistic=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy_MNB.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_train)\n",
    "    X_tr_scaled = scalerX.transform(X_train)\n",
    "    X_te_scaled = scalerX.transform(X_test)\n",
    "    \n",
    "    clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                    #has more than two categories\n",
    "    clf.fit(X_tr_scaled,y_train)\n",
    "\n",
    "    y_test_pred_logistic = clf.predict(X_te_scaled)\n",
    "    Accuracy_Logistic.append(accuracy_score(y_test,y_test_pred_logistic))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy MultinomialNB=\",np.mean(np.array(Accuracy_MNB)))\n",
    "print(\"Average 10-fold cross-validation accuracy Logistic=\",np.mean(np.array(Accuracy_Logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
