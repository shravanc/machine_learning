{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "import pandas as pd\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of classification tasks are:\n",
    "\n",
    "* Deciding whether an email is spam or not.\n",
    "* Deciding what the topic of a news article is, from a fixed list of topic areas such as \"politics,\" \"technology,\" and \"sport\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guessing gender from names\n",
    "We know that male and female names have distinctive characteristics. Names ending in a, e and i are likely to be female, while names ending in k, o, r, s and t are likely to be male.  We aim to build a classifier to model these differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "Let us define the probabilistic model.\n",
    "\n",
    "Variables and domains:\n",
    "\n",
    "$$\n",
    "G \\in \\{M,F\\}\n",
    "$$\n",
    "$$\n",
    "LL \\in \\{a,b,c,d,\\dots,z\\} %last letter in the name\n",
    "$$\n",
    "\n",
    "where LL stands for last letter in the name.\n",
    "\n",
    "We define\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(G=1) &=\\theta\\\\\n",
    "p(G=0) &=1-\\theta\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with $1$ means Female and $0$ Male. We can rewrite it in one line\n",
    "using the Bernoulli distribution.\n",
    "\n",
    "$$\n",
    "p(G=g) =\\theta^g(1-\\theta)^{1-g} \\text{ with } g \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "Let's now denote with $\\theta_{lg} $ the probability that LL is equal to the letter $l$ given that\n",
    "the gender $G$ is $g$, that is\n",
    "\n",
    "$$\n",
    "p(LL=l|G=g)=\\theta_{lg} \n",
    "$$\n",
    "we have $26\\times 2$ parameters.\n",
    "\n",
    "It is convenient to Binarize the letters (encoding) in a one-vs-all fashion, that is \n",
    "$$\n",
    "\\begin{aligned}\n",
    "a &\\rightarrow & [1,0,\\dots,0]\\\\\n",
    "b &\\rightarrow & [0,1,\\dots,0]\\\\\n",
    "... & ... & ....\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With this encoding, if we denote with $y_{l}$ the vector that has one in the l-th position \n",
    "and with $y_{al}$ is first component, $y_{bl}$ is second component etc., then \n",
    "\n",
    "$$\n",
    "p(LL=l|G=g)=\\theta_{ag}^{y_{al}}\\theta_{bg}^{y_{bl}}\\theta_{cg}^{y_{cl}}\\cdots\\theta_{zg}^{y_{zl}}\n",
    "$$\n",
    "\n",
    "which is a Categorical distribution. So we can say that the problem has $m=26$ binary features.\n",
    "\n",
    "We are interesting in computing (Bayes' rule)\n",
    "$$\n",
    "p(G=g|LL=l)=\\dfrac{p(LL=l|G=g)p(G=g)}{p(LL=l)}\n",
    "$$\n",
    "\n",
    "that is the posterior probability that the gender is $g$ when the last letter in the name is $l$\n",
    "\n",
    "For instance\n",
    "\n",
    "$$\n",
    "P(G=1|LL=a)=\\dfrac{P(LL=a|G=1)p(G=1)}{p(LL=a)}=\\dfrac{\\theta_{a1}\\theta_1}{\\theta_{a0}\\theta_0+\\theta_{a1}\\theta_1}\n",
    "$$\n",
    "\n",
    "Problem: we do not know the thetas. What do we do?\n",
    "\n",
    "We can estimate them from a Dataset of all Male and Female English givennames:\n",
    "$$\n",
    "\\mathcal{D}=\\{(a,1),(n,0),(o,0),(a,1),\\dots\\}\n",
    "$$\n",
    "\n",
    "### General recipe ML\n",
    "Maximum likelihood estimation assuming i.i.d.\n",
    "$$\n",
    "\\arg \\max_{\\theta_{lg},\\theta_{g}} \\prod_{i=1}^N p(LL=l(i)|G=g(i))P(G=g(i)) =  \\arg \\max_{\\theta_{lg},\\theta_{g}} \\prod_{i=1}^N \\theta_{l(i)g(i)}  \\theta_{g(i)}\n",
    "$$\n",
    "\n",
    "Example assume that $\\mathcal{D}=\\{(a,1),(n,0),(o,0),(a,1)\\}$, that is N=4 observations then\n",
    "$$\n",
    "\\prod_{i=1}^4 p(LL=l(i)|G=g(i))P(G=g(i))=\\theta_{a1}\\theta_1\\,\\theta_{n0}\\theta_0\\,\\theta_{o0}\\theta_0\\,\\theta_{a1}\\theta_1 =\\theta_{a1}^2\\theta_1^2\\,\\theta_{n0}\\theta_0\\,\\theta_{o0}\\theta_0\n",
    "$$\n",
    "Note that, by summing the exponent for the same base, we make the computation of this likelihood **much faster**.\n",
    "\n",
    "\n",
    "The resulting classifier is called **Multinomial Naive-Bayes estimator**, but you can see that the parameters are estimated using MLE and, therefore, the uncertainty in this estimate is not considered.\n",
    "\n",
    "Although the name of the classifier has Bayes inside, it is a general recipe ML approach because the thetas\n",
    "are estimated via MLE.\n",
    "\n",
    "The MLE estimate is\n",
    "\n",
    "$$\n",
    "\\theta_{g}=\\dfrac{n_{g}}{N} ~~\\textit{for } g=0,1\n",
    "$$\n",
    "\n",
    "where $n_{g=1}$ is the number of instances (rows) in the dataset where the class variable is equal to one and $N$\n",
    "is the total number of instances.\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\theta_{lg}=\\dfrac{n_{lg}}{n_g}\n",
    "$$\n",
    "where $n_{lg}$ is the number of instances where the letter is $l$ and the gender is $g$.\n",
    "\n",
    "**Regularisation**\n",
    "It may happen that $n_{lg}=0$ and, therefore, $\\theta_{lg}=0$. To avoid this problem, it is common to add a regularisation term (see https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
    "\n",
    "$$\n",
    "\\theta_{lg}=\\dfrac{n_{lg}+\\alpha}{n_g+\\alpha\\, m}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of features. $\\alpha=1$ is called Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return word[-1:].lower() #last_letter\n",
    "gender_features('Alessio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/shravan/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we download the names\n",
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Daisy', 'female'],\n",
       "       ['Lorry', 'female'],\n",
       "       ['Rycca', 'female'],\n",
       "       ['Carson', 'male'],\n",
       "       ['Kassia', 'female'],\n",
       "       ['Agnes', 'female'],\n",
       "       ['Thaine', 'male'],\n",
       "       ['Tamera', 'female'],\n",
       "       ['Janeen', 'female'],\n",
       "       ['Anallise', 'female']], dtype='<U15')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we make a datasets\n",
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\\\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names=np.array(labeled_names)\n",
    "labeled_names[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_letters=[gender_features(name) for name in labeled_names[:,0]]\n",
    "\n",
    "#remove NaN\n",
    "name_letters=np.array(name_letters)\n",
    "ind = np.where((name_letters!=' ')==True)[0]\n",
    "name_letters = name_letters[ind]\n",
    "\n",
    "X = name_letters#\n",
    "\n",
    "X=np.array(X).reshape(-1,1)\n",
    "y=np.where(labeled_names[ind,1]=='male',0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['y'],\n",
       "       ['y'],\n",
       "       ['a'],\n",
       "       ...,\n",
       "       ['h'],\n",
       "       ['e'],\n",
       "       ['n']], dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if our classifier is doing a good job?\n",
    "\n",
    "We can't use actually unknown outputs to check this and we can't check with outputs we trained on because it should get.\n",
    "\n",
    "In General ML, we evaluate the generalisation error, that is we test the algorithm on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(X)\n",
    "X2=  lb.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4765, 25)\n",
      "(3178, 25)\n"
     ]
    }
   ],
   "source": [
    "#we split the dataset in training and testing\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.4, random_state=42)\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "\n",
    "#we also split the original datase\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General ML Recipe algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "clf = MultinomialNB(alpha=0.1,fit_prior=True) # Note alpha\n",
    "clf.fit(X_train2, y_train2)\n",
    "y_train_pred = clf.predict(X_train2)\n",
    "y_test_pred  = clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01520287 0.98479713]\n",
      " [0.74223286 0.25776714]\n",
      " [0.52411601 0.47588399]\n",
      " ...\n",
      " [0.73783307 0.26216693]\n",
      " [0.2551471  0.7448529 ]\n",
      " [0.52411601 0.47588399]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['a'],\n",
       "       ['s'],\n",
       "       ['l'],\n",
       "       ...,\n",
       "       ['t'],\n",
       "       ['e'],\n",
       "       ['l']], dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(clf.predict_proba(X_test2))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01520287, 0.98479713]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(lb.transform(np.array([[gender_features(\"Anna\")]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "We evaluate the accuracy of the classifier.\n",
    "\n",
    "$$\n",
    "\\textit{Accuracy}= \\dfrac{\\textit{Number of correct predictions}}{\\textit{Total number of predictions}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.7672612801678909\n",
      "accuracy test set  0.7561359345500315\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy training set \", accuracy_score(y_train_pred,y_train2))\n",
    "print(\"accuracy test set \",accuracy_score(y_test_pred,y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting metric is the **confusion matrix**.\n",
    "By definition a confusion matrix $C$ is such $C_{ij}$ that is equal to the number of observations known to be in group $i$ but predicted to be in group $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 755,  374],\n",
       "       [ 401, 1648]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC=confusion_matrix(y_test2,y_test_pred)\n",
    "CC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier also returns the probability of the two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* What happens if we change the feature for instance the letter before the last?\n",
    "* What happens if we use the last two letters instead of the last one, how should you modify the code and what is the accuracy?\n",
    "* What happens if we use the last three letters, how should you modify the code and what is the accuracy?\n",
    "* What happens if we use the last four letters, how should you modify the code and what is the accuracy?\n",
    "\n",
    "Try yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
